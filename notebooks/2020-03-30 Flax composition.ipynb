{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "import jax\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from timecast.utils.numpy import ecdf\n",
    "from timecast.utils.losses import MeanSquareError\n",
    "import torch\n",
    "import matplotlib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ealstm.gaip.flood_data import FloodData\n",
    "from ealstm.gaip.utils import MSE, NSE\n",
    "\n",
    "cfg_path = \"../data/models/runs/run_2006_0032_seed444/cfg.json\"\n",
    "ea_data = pickle.load(open(\"../data/models/runs/run_2006_0032_seed444/lstm_seed444.p\", \"rb\"))\n",
    "flood_data = FloodData(cfg_path)\n",
    "\n",
    "LR_AR = 1e-5\n",
    "AR_INPUT_DIM=32\n",
    "AR_OUTPUT_DIM=1\n",
    "BATCH_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nn\n",
    "from flax import optim\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARF(nn.Module):\n",
    "    def apply(self, x, input_features, output_features, window_size, history=None):\n",
    "\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "            \n",
    "        if self.is_initializing() and history is not None:\n",
    "            x = history\n",
    "            \n",
    "        self.history = self.state(\"history\", shape=(window_size, input_features), initializer=nn.initializers.zeros)\n",
    "        self.history.value = np.vstack((self.history.value, x))[x.shape[0]:]\n",
    "        \n",
    "        y = nn.DenseGeneral(inputs=self.history.value,\n",
    "                            features=output_features,\n",
    "                            axis=(0, 1),\n",
    "                            batch_dims=(),\n",
    "                            bias=True,\n",
    "                            dtype=jnp.float32,\n",
    "                            kernel_init=nn.initializers.zeros,\n",
    "                            bias_init=nn.initializers.zeros,\n",
    "                            precision=None,\n",
    "                            name=\"linear\"\n",
    "                           )\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Take(nn.Module):\n",
    "    def apply(self, x, i):\n",
    "        return x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def apply(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plus(nn.Module):\n",
    "    def apply(self, x, z):\n",
    "        return x + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    def apply(self, x, modules, args):\n",
    "        return [module(x, **arg) for (module, arg) in zip(modules, args)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Module):\n",
    "    def apply(self, x, modules, args):\n",
    "        results = x\n",
    "        for module, arg in zip(modules, args):\n",
    "            results = module(results, **arg)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def apply(self, x, input_features, output_features, window_size, history):\n",
    "        y_arf = ARF(x=x[1],\n",
    "                    input_features=input_features,\n",
    "                    output_features=output_features,\n",
    "                    window_size=window_size,\n",
    "                    history=history\n",
    "                   )\n",
    "        return (x[0], y_arf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X, Y, optimizer, state, objective, loss_fn):\n",
    "    def _run(optstate, xy):\n",
    "        x, y = xy\n",
    "        optimizer, state = optstate\n",
    "        with nn.stateful(state) as state:\n",
    "            loss, y_hat, grad = optimizer.compute_gradients(partial(objective, x, y, loss_fn))\n",
    "            return (optimizer.apply_gradient(grad), state), y_hat\n",
    "    _, pred = jax.lax.scan(_run, (optimizer, state), (X, Y))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def residual(x, y, loss_fn, model):\n",
    "    y_hats = model(x)\n",
    "    target, y_hat, loss = y, y_hats[0], 0\n",
    "    for i in range(len(y_hats) - 1):\n",
    "        loss += loss_fn(target - y_hats[i], y_hats[i + 1])\n",
    "        target -= y_hats[i]\n",
    "        y_hat += y_hats[i + 1]\n",
    "    return loss, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xboost(x, y, loss_fn, model, reg = 1.0):\n",
    "    y_hats = model(x)\n",
    "    g = jax.grad(loss_fn)\n",
    "    u, loss = 0, 0\n",
    "    for i in range(len(y_hats)):\n",
    "        eta = (2 / (i + 2))\n",
    "        loss += g(y, u) * y_hats[i] + (reg / 2) * y_hats[i] * y_hats[i]\n",
    "        u = (1 - eta) * u + eta * y_hats[i]\n",
    "    return loss.reshape(()), u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66e4908c97a490ebeffd0ec35d5ccea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=531.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01022500 0.7921799 0.8391017 0.7921799 0.8391017\n",
      "01031500 1.034634 0.88986236 0.91340697 0.86448205\n",
      "01047000 1.2811565 0.87995404 1.0359901 0.8696394\n",
      "01052500 1.9988087 0.847376 1.2766948 0.8640735\n",
      "01054200 7.8198547 0.7346721 2.5853267 0.83819324\n",
      "01055000 4.73436 0.73252463 2.9434988 0.82058173\n",
      "01057000 0.91459924 0.87551165 2.653656 0.82842886\n",
      "01073000 0.9153604 0.86096424 2.436369 0.8324958\n",
      "01078000 0.6731362 0.8878817 2.2404542 0.8386498\n",
      "01123000 0.8993281 0.8173188 2.1063416 0.8365167\n",
      "01134500 1.715929 0.79124093 2.0708494 0.8324007\n",
      "01137500 2.376047 0.766341 2.0962827 0.8268957\n",
      "01139000 0.80212957 0.7747908 1.9967325 0.82288766\n",
      "01139800 0.84583026 0.76805705 1.9145252 0.81897116\n",
      "01142500 1.12383 0.7723832 1.8618122 0.81586534\n",
      "01144000 0.729486 0.85160327 1.7910419 0.81809896\n",
      "01162500 0.7964086 0.84338397 1.7325339 0.8195863\n",
      "01169000 2.2953029 0.7448952 1.763799 0.8154368\n",
      "01170100 1.8372197 0.7727082 1.7676632 0.8131879\n",
      "01181000 1.9368113 0.78878325 1.7761205 0.8119677\n",
      "01187300 2.769985 0.79342055 1.8234475 0.81108445\n",
      "01195100 2.0797448 0.6507689 1.8350974 0.80379736\n",
      "04296000 0.54732764 0.8664703 1.7791075 0.8065223\n",
      "01333000 0.7157472 0.8480514 1.7348008 0.80825263\n",
      "01350000 3.3880787 0.76235205 1.800932 0.80641663\n",
      "01350080 2.0443866 0.7100002 1.8102957 0.8027083\n",
      "01350140 1.9467304 0.72918826 1.8153489 0.7999854\n",
      "01365000 2.1779675 0.8103806 1.8282995 0.8003567\n",
      "01411300 0.24217708 0.857807 1.7736056 0.8023377\n",
      "01413500 1.8835168 0.77802753 1.7772694 0.8015274\n",
      "01414500 2.5095584 0.74628913 1.8008916 0.7997455\n",
      "01415000 1.2741772 0.7867589 1.7844318 0.79933965\n",
      "01423000 1.1564033 0.8146968 1.7654003 0.79980505\n",
      "01434025 4.0260167 0.7811675 1.831889 0.7992569\n",
      "01435000 3.5260613 0.81350756 1.880294 0.799664\n",
      "01439500 0.57358974 0.8828694 1.8439966 0.8019753\n",
      "01440000 0.7644013 0.83955574 1.8148185 0.802991\n",
      "01440400 1.3922086 0.79895043 1.8036971 0.80288464\n",
      "01451800 1.5841539 0.7838675 1.7980677 0.802397\n",
      "01466500 0.042933498 0.8088846 1.7541893 0.8025592\n",
      "01484100 0.66508764 0.67645395 1.727626 0.79948354\n",
      "01487000 0.20609261 0.84988475 1.691399 0.8006835\n",
      "01491000 0.68718666 0.8437203 1.6680452 0.8016844\n",
      "01510000 1.1448472 0.82818395 1.6561543 0.80228657\n",
      "01516500 3.1140468 0.5887856 1.688552 0.79754215\n",
      "01518862 2.3406444 0.61250305 1.7027279 0.79351956\n",
      "01532000 5.0607347 0.54906386 1.7741749 0.7883184\n",
      "01539000 1.0573436 0.8346026 1.759241 0.7892826\n",
      "01542810 2.486218 0.7210318 1.7740772 0.7878898\n",
      "01543000 1.320333 0.7936303 1.7650023 0.7880045\n",
      "01543500 0.83327043 0.8435682 1.7467331 0.7890941\n",
      "01544500 1.2570205 0.8319203 1.7373155 0.7899176\n",
      "01545600 0.94474727 0.8378005 1.7223614 0.79082114\n",
      "01547700 1.4619044 0.7610488 1.717538 0.7902698\n",
      "01548500 0.73403907 0.851843 1.6996562 0.79138935\n",
      "01549500 2.2679958 0.74303573 1.7098051 0.79052585\n",
      "01550000 2.9782274 0.69175375 1.7320582 0.78879297\n",
      "01552000 2.0677288 0.73766184 1.7378455 0.78791136\n",
      "01552500 4.290362 0.62505007 1.7811087 0.78515106\n",
      "01557500 1.1264625 0.7949574 1.770198 0.7853145\n",
      "01567500 2.646564 0.6186981 1.7845646 0.78258306\n",
      "01568000 2.0386553 0.6997416 1.7886628 0.7812469\n",
      "01580000 0.8525555 0.6547979 1.773804 0.7792398\n",
      "01583500 0.7408751 0.58709216 1.7576644 0.7762375\n",
      "01586610 1.1777049 0.6345308 1.748742 0.7740574\n",
      "01591400 1.853165 0.52659243 1.7503241 0.77030796\n",
      "01594950 1.9973847 0.759602 1.7540116 0.7701481\n",
      "01596500 1.886167 0.72829854 1.755955 0.7695327\n",
      "01605500 1.0443232 0.7312882 1.7456416 0.7689784\n",
      "01606500 1.2208343 0.75767404 1.7381443 0.76881695\n",
      "01632000 3.7986705 0.60135853 1.7671657 0.76645833\n",
      "01632900 1.4309392 0.5978098 1.7624959 0.764116\n",
      "01634500 2.2347467 0.654806 1.7689652 0.7626186\n",
      "01638480 2.1834917 0.59325683 1.7745669 0.76032996\n",
      "01639500 1.3366051 0.6667125 1.7687272 0.75908166\n",
      "01644000 1.0313833 0.731468 1.7590253 0.7587183\n",
      "01664000 1.320801 0.70744026 1.7533342 0.7580524\n",
      "01666500 2.0416818 0.76098776 1.757031 0.75809\n",
      "01667500 1.360312 0.8176646 1.7520092 0.7588442\n",
      "01669000 0.437363 0.6801064 1.7355763 0.75786\n",
      "01669520 0.4500803 0.8623376 1.7197058 0.7591498\n",
      "02011400 0.6434788 0.8098392 1.7065814 0.75976795\n",
      "02013000 1.0871013 0.78782946 1.6991174 0.7601061\n",
      "02014000 0.66101944 0.8117841 1.6867592 0.7607213\n",
      "02015700 1.6577953 0.7373508 1.6864187 0.76044637\n",
      "02016000 0.95694447 0.8144643 1.6779363 0.7610744\n",
      "02017500 0.862717 0.8078279 1.6685658 0.7616119\n",
      "02018000 0.9302834 0.8028246 1.6601762 0.7620802\n",
      "02027000 2.9226713 0.6553502 1.6743617 0.7608809\n",
      "02027500 6.378175 0.5747938 1.7266263 0.7588133\n",
      "02028500 2.0933146 0.7267333 1.7306559 0.75846076\n",
      "02038850 2.589849 0.4939068 1.7399949 0.7555852\n",
      "02046000 1.633225 0.43316758 1.7388468 0.75211835\n",
      "02051500 0.57797486 0.8094017 1.726497 0.75272775\n",
      "02053200 0.97507954 0.802351 1.7185874 0.7532501\n",
      "02053800 1.1217797 0.63691825 1.7123708 0.7520383\n",
      "02055100 1.413399 0.60018575 1.7092885 0.7504727\n",
      "02056900 1.0553852 0.66702104 1.702616 0.74962115\n",
      "02059500 1.3638672 0.6053959 1.6991942 0.7481644\n",
      "02064000 3.1772199 0.5999298 1.7139746 0.74668205\n",
      "02065500 2.2874177 0.4768772 1.7196522 0.7440107\n",
      "02069700 0.55065465 0.7512052 1.7081915 0.74408126\n",
      "02070000 0.9710032 0.63195014 1.7010343 0.7429926\n",
      "02074500 1.6883442 0.7433109 1.7009122 0.7429956\n",
      "02077200 9.825093 0.4585461 1.7782854 0.7402867\n",
      "02081500 1.7158887 0.8024892 1.7776967 0.7408735\n",
      "02082950 3.54647 0.52935696 1.7942274 0.7388967\n",
      "02092500 0.9596568 0.840193 1.7864999 0.73983467\n",
      "02096846 3.067625 0.6392623 1.7982533 0.7389119\n",
      "02102908 0.6099705 0.56924975 1.7874507 0.73736954\n",
      "02108000 0.50708497 0.90027857 1.7759157 0.73883724\n",
      "02111180 3.3775225 0.69908094 1.7902158 0.7384823\n",
      "02111500 1.7264855 0.536374 1.7896521 0.7366937\n",
      "02112120 1.3365854 0.6156368 1.7856777 0.7356317\n",
      "02112360 1.0386612 0.4902801 1.7791818 0.7334982\n",
      "02118500 0.96514446 0.74901867 1.7721643 0.733632\n",
      "02125000 5.6152987 0.58858466 1.8050117 0.73239243\n",
      "02128000 5.142896 0.5674669 1.8332988 0.73099464\n",
      "02137727 2.2162735 0.66214705 1.8365171 0.7304161\n",
      "02140991 2.6121576 0.677989 1.8429809 0.7299792\n",
      "02143000 1.7606819 0.6861115 1.8423005 0.72961664\n",
      "02143040 2.9573514 0.585224 1.8514403 0.72843313\n",
      "02149000 0.9255848 0.72975445 1.8439131 0.7284439\n",
      "02152100 1.8672017 0.5087841 1.844101 0.7266725\n",
      "02177000 0.5901465 0.9050578 1.8340694 0.7280995\n",
      "02178400 1.5840893 0.8133897 1.8320853 0.72877634\n",
      "02193340 1.8825446 0.7336366 1.8324827 0.72881466\n",
      "02196000 0.8299293 0.8193793 1.8246502 0.7295222\n",
      "02198100 3.7085755 0.6180834 1.8392543 0.72865826\n",
      "02202600 0.88702506 0.8148589 1.8319293 0.7293214\n",
      "02212600 1.8092383 0.77987784 1.8317561 0.7297073\n",
      "02215100 0.87137675 0.8136481 1.8244805 0.7303432\n",
      "02216180 1.9516193 0.7624544 1.8254365 0.7305847\n",
      "02221525 0.38131112 0.8905687 1.8146595 0.73177856\n",
      "02231000 0.12507185 0.94851434 1.8021438 0.7333841\n",
      "02245500 1.4366783 0.6604894 1.7994566 0.73284805\n",
      "02246000 1.017353 0.8328676 1.7937479 0.73357815\n",
      "02296500 0.102483734 0.9459169 1.7814924 0.73511684\n",
      "02297155 1.8123703 0.4891039 1.7817146 0.73334694\n",
      "02297310 0.9262933 0.7590846 1.7756044 0.7335308\n",
      "02298123 0.1907659 0.91300803 1.7643644 0.73480374\n",
      "02298608 1.7898089 0.82323235 1.7645435 0.7354265\n",
      "02299950 7.3708773 0.29282898 1.8037487 0.73233134\n",
      "02300700 7.0752077 0.60064876 1.840356 0.7314169\n",
      "02342933 7.183501 0.62501585 1.8772053 0.7306831\n",
      "02349900 1.4635935 0.79244846 1.8743722 0.7311061\n",
      "02350900 0.34103557 0.86500376 1.8639417 0.732017\n",
      "02361000 2.5578496 0.7387304 1.86863 0.73206234\n",
      "02363000 0.9450845 0.86202276 1.8624316 0.73293453\n",
      "02369800 5.1547694 0.7912231 1.8843807 0.7333231\n",
      "02371500 0.73410004 0.8545444 1.876763 0.7341259\n",
      "02372250 1.5871106 0.799482 1.8748574 0.73455596\n",
      "02374500 1.1156898 0.7202954 1.8698953 0.73446274\n",
      "02381600 2.3420274 0.69370973 1.8729613 0.7341981\n",
      "02384540 6.845128 0.70656544 1.9050399 0.7340198\n",
      "02395120 2.4747705 0.71452713 1.9086918 0.73389494\n",
      "02415000 1.4673787 0.85280937 1.905881 0.73465234\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "mses = []\n",
    "nses = []\n",
    "\n",
    "for X, y, basin in tqdm.tqdm(flood_data.generator(), total=len(flood_data.basins)):\n",
    "    with nn.stateful() as state:\n",
    "#         model_def = Residual.partial(input_features=32,\n",
    "#                                      output_features=1,\n",
    "#                                      window_size=270,\n",
    "#                                      history=X[:flood_data.cfg[\"seq_length\"]-1]\n",
    "#                                     )\n",
    "        lstm = Sequential.partial(modules=[Take, Identity], args=[{\"i\": 0}, {}])\n",
    "        \n",
    "        arf = Sequential.partial(modules=[Take, ARF], args=[{\"i\": 1}, {\"input_features\": 32, \"output_features\": 1, \"window_size\": 270, \"history\": X[:flood_data.cfg[\"seq_length\"]-1]}])\n",
    "        \n",
    "        model_def = Ensemble.partial(modules=[lstm, arf], args=[{}, {}])\n",
    "        ys, params = model_def.init_by_shape(jax.random.PRNGKey(0), [(1, 32)])\n",
    "        model = nn.Model(model_def, params)\n",
    "    optim_def = optim.GradientDescent(learning_rate=1e-5)\n",
    "    optimizer = optim_def.create(model)\n",
    "    \n",
    "    # NOTE: difference in indexing convention, so need to pad one row\n",
    "    X_t = X[flood_data.cfg[\"seq_length\"]-1:]\n",
    "    Y_lstm = np.array(ea_data[basin].qsim)\n",
    "    Y = np.array(ea_data[basin].qobs).reshape(-1, 1)\n",
    "    \n",
    "    Y_hat = run((Y_lstm, X_t), Y, optimizer, state, residual, lambda x, y: jnp.square(x-y).mean())\n",
    "    \n",
    "    mse = MSE(Y, Y_hat)\n",
    "    nse = NSE(Y, Y_hat)\n",
    "    results[basin] = {\n",
    "        \"mse\": mse,\n",
    "        \"nse\": nse,\n",
    "        \"count\": X.shape[0],\n",
    "        \"avg_mse\": np.mean(np.array(mses)),\n",
    "        \"avg_nse\": np.mean(np.array(nses))\n",
    "    }\n",
    "    mses.append(mse)\n",
    "    nses.append(nse)\n",
    "    print(basin, mse, nse, np.mean(np.array(mses)), np.mean(np.array(nses)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ea_data[basin].qsim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    pass\n",
    "class B(A):\n",
    "    pass\n",
    "class C(B):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issubclass(type(c), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
