{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ttic.uchicago.edu/~tewari/lectures/lecture4.pdf\n",
    "\n",
    "# from tigerforecast.batch.camels_dataloader import CamelsTXT\n",
    "# for basin in tqdm.tqdm(basins):\n",
    "#     usgs_val = CamelsTXT(basin=basin, concat_static=True)\n",
    "#     for data, targets in usgs_val.sequential_batches(batch_size=5000):\n",
    "#         pickle.dump(data, open(\"../data/flood/test/{}.pkl\".format(basin), \"wb\"))\n",
    "#         pickle.dump(targets, open(\"../data/flood/qobs/{}.pkl\".format(basin), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = pickle.load(open(\"../data/flood/meta.pkl\", \"rb\"))[\"basins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tigerforecast.utils.download_tools import get_tigerforecast_dir\n",
    "basin_to_yhats_LSTM = pickle.load(open(os.path.join(get_tigerforecast_dir(), \"flood_prediction\", \"basin_to_yhats_LSTM\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dsuo/src/TigerForecast/tigerforecast/flood_prediction/basin_to_yhats_LSTM'"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(get_tigerforecast_dir(), \"flood_prediction\", \"basin_to_yhats_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Optimizers should apply to all children unless children have specified version\n",
    "# - hierarchical parameters?\n",
    "# - Tree flatten is very crude (only applies to params)\n",
    "# - How to identify params (right now just ndarray)\n",
    "# - Pass class directly to jax\n",
    "# - How to handle buffers vs parameters\n",
    "# - Users can do bad things with naming\n",
    "import inspect\n",
    "\n",
    "def tree_flatten(module):\n",
    "    leaves, aux = jax.tree_util.tree_flatten(module.get_param_tree())\n",
    "    aux = {\n",
    "        \"treedef\": aux,\n",
    "        \"arguments\": module.arguments,\n",
    "        \"attrs\": module.attrs,\n",
    "        \"class\": module.__class__,\n",
    "    }\n",
    "    return leaves, aux\n",
    "\n",
    "def tree_unflatten(aux, leaves):\n",
    "    module = aux[\"class\"](*aux[\"arguments\"].args, **aux[\"arguments\"].kwargs)\n",
    "    module.set_param_tree(jax.tree_util.tree_unflatten(aux[\"treedef\"], leaves))\n",
    "    for attr in aux[\"attrs\"]:\n",
    "        if attr in module.__dict__[\"params\"]:\n",
    "            module.__dict__[attr] = module.__dict__[\"params\"][attr]\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        obj = object.__new__(cls)\n",
    "        obj.__setattr__(\"attrs\", set())\n",
    "        obj.__setattr__(\"modules\", {})\n",
    "        obj.__setattr__(\"params\", {})\n",
    "        obj.__setattr__(\"arguments\", inspect.signature(obj.__init__).bind(*args))\n",
    "        obj.arguments.apply_defaults()\n",
    "\n",
    "        return obj\n",
    "    \n",
    "    @classmethod\n",
    "    def __init_subclass__(cls, *args, **kwargs):\n",
    "        super().__init_subclass__(*args, **kwargs)\n",
    "        jax.tree_util.register_pytree_node(cls, tree_flatten, tree_unflatten)\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        self.__dict__[name] = value\n",
    "        self.attrs.add(name)\n",
    "\n",
    "        if isinstance(value, Module):\n",
    "            self.__dict__[\"modules\"][name] = value\n",
    "        elif isinstance(value, jnp.ndarray):\n",
    "            self.__dict__[\"params\"][name] = value\n",
    "    \n",
    "    def get_param_tree(self):\n",
    "        params = self.params\n",
    "        for name, module in self.modules.items():\n",
    "            params[name] = module.get_param_tree()    \n",
    "        return params\n",
    "    \n",
    "    def set_param_tree(self, tree):\n",
    "        for param in self.params:\n",
    "            self.params[param] = tree[param]\n",
    "            self.__dict__[param] = tree[param]\n",
    "        for name, module in self.modules.items():\n",
    "            module.set_param_tree(tree[name])\n",
    "            \n",
    "    def add_module(self, module, name=None):\n",
    "        counter = 0\n",
    "        while name is None or name in self.__dict__[\"modules\"]:\n",
    "            name = \"{}_{}\".format(type(module).__name__, counter)\n",
    "            counter += 1      \n",
    "        self.__dict__[\"modules\"][name] = module\n",
    "        \n",
    "    def add_param(self, param, name):\n",
    "        counter = 0\n",
    "        while name is None or name in self.__dict__[\"params\"]:\n",
    "            name = \"{}_{}\".format(name, counter)\n",
    "            counter += 1 \n",
    "        self.__dict__[\"params\"][name] = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self,\n",
    "                 loss_fn=lambda pred, true: jnp.square(pred - true).mean(),\n",
    "                 learning_rate=0.0001,\n",
    "                 project_threshold={}):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.project_threshold = project_threshold\n",
    "        \n",
    "    def update(self, module, params, x, y):\n",
    "        grad = jax.jit(jax.grad(lambda module, x, y: self.loss_fn(module(x), y)))(module, x, y)\n",
    "        new_params = {k:w - self.learning_rate * grad.params[k] for (k, w) in params.items()}\n",
    "        \n",
    "        for k, param in new_params.items():\n",
    "            norm = jnp.linalg.norm(new_params[k])\n",
    "            new_params[k] = jax.lax.cond(norm > self.project_threshold[k],\n",
    "                                          new_params[k],\n",
    "                                          lambda x : (self.project_threshold[k]/norm) * x,\n",
    "                                          new_params[k],\n",
    "                                          lambda x : x)\n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MW:\n",
    "    def __init__(self, eta=0.008):\n",
    "        self.eta = eta\n",
    "        self.grad = jax.jit(jax.grad(lambda W, preds, y: jnp.square(jnp.dot(W, preds) - y).sum()))\n",
    "        \n",
    "    def update(self, params, x, y):\n",
    "        grad = self.grad(params, x, y)\n",
    "        new_params = params * jnp.exp(-1 * self.eta * grad)\n",
    "        return new_params / new_params.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR(Module):\n",
    "    def __init__(self, input_dim=32, output_dim=1, history_len=270):\n",
    "        self.kernel = jnp.zeros((history_len, input_dim, output_dim))\n",
    "        self.bias = jnp.zeros((output_dim, 1))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.tensordot(self.kernel, x, ([0,1],[0,1])) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting(Module):\n",
    "    def __init__(self, N, input_dim=32, output_dim=1, history_len=270):\n",
    "        for i in range(N):\n",
    "            self.add_module(AR(input_dim=input_dim, output_dim=output_dim, history_len=history_len))\n",
    "            \n",
    "        self.W = jnp.ones(N) / N\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        pred, preds = 0, []\n",
    "        for i, (name, submodule) in enumerate(self.modules.items()):\n",
    "            pred_i = submodule(x).squeeze()\n",
    "            preds.append(pred_i)\n",
    "            pred += self.W[i] * pred_i\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/531 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01022500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/531 [00:02<19:10,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.484344\n",
      "01031500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/531 [00:04<38:40,  4.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-425-a2bbc9ec4028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_LSTM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY_BOOST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/skgaip/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_forward_method\u001b[0;34m(attrname, self, fun, *args)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0m_forward_to_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_forward_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/skgaip/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_lexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_force\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for basin in tqdm.tqdm(basins):\n",
    "    bias_threshold = 1e-4\n",
    "    eta = 0.008\n",
    "\n",
    "    SGDs = [SGD(\n",
    "        learning_rate=lr,\n",
    "        project_threshold={\"kernel\": kernel_threshold, \"bias\": bias_threshold})\n",
    "                  for kernel_threshold, lr in [\n",
    "            (0.03, 2e-5),\n",
    "            (0.05, 2e-5),\n",
    "            (0.07, 2e-5),\n",
    "            (0.09, 2e-5),\n",
    "        ]]\n",
    "    \n",
    "    N = len(SGDs)\n",
    "\n",
    "    module = GradientBoosting(N)\n",
    "\n",
    "    Y_LSTM = jnp.array(basin_to_yhats_LSTM[basin])\n",
    "    X = pickle.load(open(\"../data/flood/test/{}.pkl\".format(basin), \"rb\"))\n",
    "    Y = pickle.load(open(\"../data/flood/qobs/{}.pkl\".format(basin), \"rb\"))\n",
    "    \n",
    "    def loop(module, xy):\n",
    "        x, y = xy\n",
    "\n",
    "        preds = jnp.asarray(module(x))\n",
    "        pred = 0\n",
    "        \n",
    "        for i, (name, submodule) in enumerate(module.modules.items()):\n",
    "            submodule.params = SGDs[i].update(submodule, submodule.params, x, y - pred)\n",
    "            pred += module.W[i] * preds[i]\n",
    "        \n",
    "        module.W = MW().update(module.W, preds, y)\n",
    "        \n",
    "        return module, pred\n",
    "        \n",
    "    Y_RESID = Y - Y_LSTM\n",
    "    module, Y_BOOST = jax.lax.scan(loop, module, (X, Y_RESID))\n",
    "    \n",
    "#     for x, y in zip(X, Y_RESID):\n",
    "#         module, y_hat = loop(module, (x, y))\n",
    "    \n",
    "    Y_BOOST = jnp.asarray(Y_BOOST).squeeze()\n",
    "    loss = ((Y - (Y_LSTM + Y_BOOST)) ** 2).mean()\n",
    "    \n",
    "    print(basin, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
