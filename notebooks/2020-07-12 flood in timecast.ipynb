{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import timecast as tc\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r gs://skgaip/data/flood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = pickle.load(open(\"data/flood/meta.pkl\", \"rb\"))[\"basins\"]\n",
    "basin_to_yhats_LSTM = pickle.load(open(\"data/flood/tigerforecast/lstm.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self,\n",
    "                 loss_fn=lambda pred, true: jnp.square(pred - true).mean(),\n",
    "                 learning_rate=0.0001,\n",
    "                 project_threshold={}):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.project_threshold = project_threshold\n",
    "        \n",
    "    def update(self, module, params, x, y):\n",
    "        grad = jax.jit(jax.grad(lambda module, x, y: self.loss_fn(module(x), y)))(module, x, y)\n",
    "        new_params = {k:w - self.learning_rate * grad.params[k] for (k, w) in params.items()}\n",
    "        \n",
    "        for k, param in new_params.items():\n",
    "            norm = jnp.linalg.norm(new_params[k])\n",
    "            new_params[k] = jax.lax.cond(norm > self.project_threshold[k],\n",
    "                                          new_params[k],\n",
    "                                          lambda x : (self.project_threshold[k]/norm) * x,\n",
    "                                          new_params[k],\n",
    "                                          lambda x : x)\n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeWeights:\n",
    "    def __init__(self, eta=0.008):\n",
    "        self.eta = eta\n",
    "        self.grad = jax.jit(jax.grad(lambda W, preds, y: jnp.square(jnp.dot(W, preds) - y).sum()))\n",
    "        \n",
    "    def update(self, module, params, x, y):\n",
    "        grad = self.grad(params, x, y)\n",
    "        new_params = params * jnp.exp(-1 * self.eta * grad)\n",
    "        return new_params / new_params.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR(tc.Module):\n",
    "    def __init__(self, input_dim=32, output_dim=1, history_len=270):\n",
    "        self.kernel = jnp.zeros((history_len, input_dim, output_dim))\n",
    "        self.bias = jnp.zeros((output_dim, 1))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.tensordot(self.kernel, x, ([0,1],[0,1])) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting(tc.Module):\n",
    "    def __init__(self, N, input_dim=32, output_dim=1, history_len=270):\n",
    "        for i in range(N):\n",
    "            self.add_module(AR(input_dim=input_dim, output_dim=output_dim, history_len=history_len))\n",
    "            \n",
    "        self.W = jnp.ones(N) / N\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        pred, preds = 0, []\n",
    "        for i, (name, submodule) in enumerate(self.modules.items()):\n",
    "            pred_i = submodule(x).squeeze()\n",
    "            preds.append(pred_i)\n",
    "            pred += self.W[i] * pred_i\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_threshold = 1e-4\n",
    "eta = 0.008\n",
    "\n",
    "SGDs = [SGD(\n",
    "    learning_rate=learning_rate,\n",
    "    project_threshold={\n",
    "        \"kernel\": kernel_threshold,\n",
    "        \"bias\": bias_threshold\n",
    "    })\n",
    "    for kernel_threshold, learning_rate in [\n",
    "        (0.03, 2e-5),\n",
    "        (0.05, 2e-5),\n",
    "        (0.07, 2e-5),\n",
    "        (0.09, 2e-5),\n",
    "]]\n",
    "\n",
    "MW = MultiplicativeWeights(eta=eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(basin):\n",
    "    N = len(SGDs)\n",
    "    \n",
    "    model = GradientBoosting(N)\n",
    "    \n",
    "    Y_LSTM = jnp.array(basin_to_yhats_LSTM[basin])\n",
    "    X = pickle.load(open(\"data/flood/test/{}.pkl\".format(basin), \"rb\"))\n",
    "    Y = pickle.load(open(\"data/flood/qobs/{}.pkl\".format(basin), \"rb\"))\n",
    "    \n",
    "    def loop(model, xy):\n",
    "        x, y = xy\n",
    "\n",
    "        preds = jnp.asarray(model(x))\n",
    "        pred = 0\n",
    "        \n",
    "        for i, (name, module) in enumerate(model.modules.items()):\n",
    "            module.params = SGDs[i].update(module, module.params, x, y - pred)\n",
    "            pred += model.W[i] * preds[i]\n",
    "        \n",
    "        model.W = MW.update(model, model.W, preds, y)\n",
    "        \n",
    "        return model, pred\n",
    "    \n",
    "    Y_RESID = Y - Y_LSTM\n",
    "    module, Y_BOOST = jax.lax.scan(loop, model, (X, Y_RESID))\n",
    "    \n",
    "    # for x, y in zip(X, Y_RESID):\n",
    "        # module, y_hat = loop(module, (x, y))\n",
    "    \n",
    "    Y_BOOST = jnp.asarray(Y_BOOST).squeeze()\n",
    "    loss = ((Y - (Y_LSTM + Y_BOOST)) ** 2).mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for basin in tqdm.tqdm(basins):\n",
    "    loss = predict(basin)\n",
    "    print(basin, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
