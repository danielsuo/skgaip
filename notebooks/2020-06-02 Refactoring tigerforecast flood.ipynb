{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ttic.uchicago.edu/~tewari/lectures/lecture4.pdf\n",
    "\n",
    "# from tigerforecast.batch.camels_dataloader import CamelsTXT\n",
    "# for basin in tqdm.tqdm(basins):\n",
    "#     usgs_val = CamelsTXT(basin=basin, concat_static=True)\n",
    "#     for data, targets in usgs_val.sequential_batches(batch_size=5000):\n",
    "#         pickle.dump(data, open(\"../data/flood/test/{}.pkl\".format(basin), \"wb\"))\n",
    "#         pickle.dump(targets, open(\"../data/flood/qobs/{}.pkl\".format(basin), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = pickle.load(open(\"../data/flood/meta.pkl\", \"rb\"))[\"basins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsuo/miniconda3/envs/skgaip/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "from tigerforecast.utils.download_tools import get_tigerforecast_dir\n",
    "basin_to_yhats_LSTM = pickle.load(open(os.path.join(get_tigerforecast_dir(), \"flood_prediction\", \"basin_to_yhats_LSTM\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self,\n",
    "                 pred=None,\n",
    "                 loss_fn=lambda pred, true: np.square(pred - true).mean(),\n",
    "                 learning_rate=0.0001,\n",
    "                 project_threshold={}):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.project_threshold = project_threshold\n",
    "        self.grad = jax.jit(jax.grad(lambda params, x, y: self.loss_fn(pred(params, x), y)))\n",
    "\n",
    "    def update(self, params, x, y):\n",
    "        grad = self.grad(params, x, y)\n",
    "        new_params = {k:w - self.learning_rate * grad[k] for (k, w) in params.items()}\n",
    "        \n",
    "        for k, param in new_params.items():\n",
    "            norm = np.linalg.norm(new_params[k])\n",
    "            new_params[k] = jax.lax.cond(norm > self.project_threshold[k],\n",
    "                                          new_params[k],\n",
    "                                          lambda x : (self.project_threshold[k]/norm) * x,\n",
    "                                          new_params[k],\n",
    "                                          lambda x : x)\n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MW:\n",
    "    def __init__(self, eta=0.008):\n",
    "        self.eta = eta\n",
    "        self.grad = jax.jit(jax.grad(lambda W, preds, y: np.square(np.dot(W, preds) - y).sum()))\n",
    "        \n",
    "    def update(self, params, x, y):\n",
    "        grad = self.grad(params, x, y)\n",
    "        new_params = params * np.exp(-1 * self.eta * grad)\n",
    "        return new_params / new_params.sum()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR:\n",
    "    def __init__(self, input_dim=1, output_dim=1, history_len=32):\n",
    "        self.params = {'W_lnm': np.zeros((history_len, input_dim, output_dim)), 'b': np.zeros((output_dim, 1))}\n",
    "        \n",
    "    def __call__(self, params, x):\n",
    "        return np.tensordot(params['W_lnm'], x, ([0,1],[0,1])) + params['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class Gradient_boosting:\n",
    "    def __init__(self, methods):\n",
    "        self.N = len(methods)\n",
    "        self.methods = methods\n",
    "        self.params = {\"W\": np.ones(self.N) / self.N, \"children\": [method.params for method in self.methods]}\n",
    "        self.loss_W_grad = jax.jit(jax.grad(lambda W, preds, y: np.square(np.dot(W, preds) - y).sum()))\n",
    "        \n",
    "    def __call__(self, params, x):\n",
    "        pred, preds = 0, []\n",
    "        for i in range(self.N):\n",
    "            pred_i = self.methods[i](params[\"children\"][i], x).squeeze()\n",
    "            preds.append(pred_i)\n",
    "            pred += params[\"W\"][i] * pred_i\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/531 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01022500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/531 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.484344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for basin in tqdm.tqdm(basins):\n",
    "    SEQUENCE_LENGTH = 270\n",
    "    INPUT_DIM = 32\n",
    "\n",
    "    b_threshold = 1e-4\n",
    "    eta = 0.008\n",
    "\n",
    "    W_lr_best_pairs = [\n",
    "            (0.03, 2e-5),\n",
    "            (0.05, 2e-5),\n",
    "            (0.07, 2e-5),\n",
    "            (0.09, 2e-5),\n",
    "        ]\n",
    "\n",
    "    methods, optimizers = [], []\n",
    "\n",
    "    for W_threshold, lr in W_lr_best_pairs:\n",
    "        project_threshold = {\"W_lnm\": W_threshold, \"b\": b_threshold}\n",
    "        \n",
    "        method_ar = AR(\n",
    "            input_dim=INPUT_DIM,\n",
    "            output_dim=1,\n",
    "            history_len=SEQUENCE_LENGTH\n",
    "        )\n",
    "        optim_ar = SGD(pred = method_ar, learning_rate=lr, project_threshold=project_threshold)\n",
    "        optimizers.append(optim_ar)\n",
    "        methods.append(method_ar)\n",
    "\n",
    "    method_boosting = Gradient_boosting(methods)\n",
    "\n",
    "    yhats_LSTM = np.array(basin_to_yhats_LSTM[basin])\n",
    "    X = pickle.load(open(\"../data/flood/test/{}.pkl\".format(basin), \"rb\"))\n",
    "    Y = pickle.load(open(\"../data/flood/qobs/{}.pkl\".format(basin), \"rb\"))\n",
    "    \n",
    "    def loop(params, xy):\n",
    "        x, y = xy\n",
    "        preds = method_boosting(params, x)\n",
    "        \n",
    "        pred = 0\n",
    "        for i in range(method_boosting.N):\n",
    "            params[\"children\"][i] = optimizers[i].update(params[\"children\"][i], x, y - pred)\n",
    "            pred += params[\"W\"][i] * preds[i]\n",
    "            \n",
    "        params[\"W\"] = MW().update(params[\"W\"], np.asarray(preds), y)\n",
    "        \n",
    "        return params, pred\n",
    "        \n",
    "    y_true = Y - yhats_LSTM\n",
    "    method_boosting.params, y_pred_ar = jax.lax.scan(loop, method_boosting.params, (X, y_true))\n",
    "    \n",
    "# TODO\n",
    "# - Figure out hierarchy\n",
    "# - Register nodes\n",
    "# - Pass in directly to jax\n",
    "# - Figure out how to automatically do hierarchy/figure out nodes\n",
    "# - Optimizers should apply to all children unless children have specified version\n",
    "\n",
    "#     W = np.ones(4) / 4\n",
    "#     params = [method.params for method in method_boosting.methods]\n",
    "#     y_pred_ar = []\n",
    "#     for x, y in zip(X, y_true):\n",
    "#         print(W)\n",
    "#         (W, params), y_hat = method_boosting((W, params), (x, y))\n",
    "#         y_pred_ar.append(y_hat)\n",
    "\n",
    "    y_pred_ar = np.asarray(y_pred_ar)\n",
    "    yhats = yhats_LSTM + y_pred_ar.squeeze()\n",
    "    loss = ((Y - yhats) ** 2).mean()\n",
    "    \n",
    "    print(basin, loss)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats_LSTM.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
