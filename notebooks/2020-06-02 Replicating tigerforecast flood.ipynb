{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import tqdm\n",
    "\n",
    "from tigerforecast.utils.optimizers.losses import batched_mse, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ttic.uchicago.edu/~tewari/lectures/lecture4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = pickle.load(open(\"../data/flood/meta.pkl\", \"rb\"))[\"basins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tigerforecast.utils.download_tools import get_tigerforecast_dir\n",
    "basin_to_yhats_LSTM = pickle.load(open(os.path.join(get_tigerforecast_dir(), \"flood_prediction\", \"basin_to_yhats_LSTM\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tigerforecast.batch.camels_dataloader import CamelsTXT\n",
    "# for basin in tqdm.tqdm(basins):\n",
    "#     usgs_val = CamelsTXT(basin=basin, concat_static=True)\n",
    "#     for data, targets in usgs_val.sequential_batches(batch_size=5000):\n",
    "#         pickle.dump(data, open(\"../data/flood/test/{}.pkl\".format(basin), \"wb\"))\n",
    "#         pickle.dump(targets, open(\"../data/flood/qobs/{}.pkl\".format(basin), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SGD optimizer\n",
    "'''\n",
    "from tigerforecast.utils.optimizers.core import Optimizer\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Description: Stochastic Gradient Descent optimizer.\n",
    "    Args:\n",
    "        pred (function): a prediction function implemented with jax.numpy \n",
    "        loss (function): specifies loss function to be used; defaults to MSE\n",
    "        learning_rate (float): learning rate\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def __init__(self, pred=None, loss=mse, learning_rate=0.0001, include_x_loss=False, hyperparameters={}, clip_grad=False, clip_threshold={}):\n",
    "        self.initialized = False\n",
    "        self.lr = learning_rate\n",
    "        self.include_x_loss = include_x_loss\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.pred = pred\n",
    "        self.loss = loss\n",
    "        if self._is_valid_pred(pred, raise_error=False) and self._is_valid_loss(loss, raise_error=False):\n",
    "            self.set_predict(pred, loss=loss)\n",
    "        self.t = 0\n",
    "        self.grad_norm_sum = {}\n",
    "        self.clip_grad = clip_grad\n",
    "        self.clip_threshold = clip_threshold\n",
    "\n",
    "    def update_scan(self, params, metadata, x, y, loss=None):\n",
    "        assert self.initialized\n",
    "        assert type(params) == dict\n",
    "        grad = self.gradient(params, x, y, loss=loss)\n",
    "        if self.clip_grad:\n",
    "            for k,w in grad.items():\n",
    "                grad[k] = jax.lax.cond(np.linalg.norm(grad[k]) > self.clip_threshold[k],\n",
    "                                       None,\n",
    "                                       lambda x : (self.clip_threshold[k]/np.linalg.norm(grad[k])) *  grad[k],\n",
    "                                       grad[k],\n",
    "                                       lambda x : x)\n",
    "               \n",
    "        new_params = {k:w - self.lr * grad[k] for (k, w) in params.items()}\n",
    "        return new_params, metadata\n",
    "\n",
    "    def initialize_metadata(self, params):\n",
    "        self.metadata = {'t': 0, 'grad_norm_sum': {k: 0.0 for k,v in params.items()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stateless AR method\n",
    "\"\"\"\n",
    "import jax.experimental.stax as stax\n",
    "import tigerforecast\n",
    "from tigerforecast.utils.random import generate_key\n",
    "from tigerforecast.methods import Method\n",
    "\n",
    "\n",
    "class ARStateless_scan(Method):\n",
    "    \"\"\"\n",
    "    Description: Produces outputs from a randomly initialized seq2seq LSTM neural network.\n",
    "                 Supposed to be used in batch seq2seq mode. Not online mode. \n",
    "    \"\"\"\n",
    "\n",
    "    compatibles = set(['TimeSeries'])\n",
    "\n",
    "    def __init__(self, n=1, m=1, l=32, optimizer=None, activation=None, project_threshold={}, scan_mode=True):\n",
    "        \"\"\"\n",
    "        Description: Randomly initialize the Stateless AR.\n",
    "        Args:\n",
    "            m (int): Observation/output dimension.\n",
    "            n (int): Input action dimension.\n",
    "            l (int): Length of memory for update step purposes.\n",
    "            optimizer (instance of Optimizer Class): optimizer choice\n",
    "            acitvation: activation function to compose with predict\n",
    "        \"\"\"\n",
    "        self.T = 0\n",
    "        self.initialized = True\n",
    "        self.n, self.m, self.l = n, m, l\n",
    "\n",
    "        # initialize parameters\n",
    "        glorot_init = stax.glorot() # returns a function that initializes weights\n",
    "        # W_lnm = glorot_init(generate_key(), (l, m, n)) # maps l inputs to output\n",
    "        W_lnm = np.zeros((l,n,m))\n",
    "        b = np.zeros((m, 1)) # bias \n",
    "        self.params = {'W_lnm': W_lnm, 'b': b}\n",
    "        self.metadata = {'x': np.zeros((self.l, self.n))}\n",
    "        self.activation = activation\n",
    "        self.project_threshold = project_threshold\n",
    "        self.params_norm_sum = {'W_lnm': 0.0, 'b': 0.0}\n",
    "        self.t = 0\n",
    "\n",
    "        \"\"\" private helper methods\"\"\"\n",
    "\n",
    "        @jax.jit\n",
    "        def _predict(params, x):\n",
    "            ### TODO - einsum is not needed here\n",
    "            # y = np.einsum('ijk,ij->k', params['W_lnm'], x) + params['b']\n",
    "            # print(\"--------------- ARSTATELESS_SCAN PREDICT ----------------\")\n",
    "            # print(\"x.shape = \" + str(x.shape))\n",
    "            y = np.tensordot(params['W_lnm'], x, ([0,1],[0,1])) + params['b']\n",
    "            if self.activation:\n",
    "                y = self.activation(y)\n",
    "            return y\n",
    "\n",
    "        self.transform = lambda x: float(x) if (self.m == 1) else x\n",
    "        # self._predict = jax.vmap(_predict, in_axes=(None, 0))\n",
    "        self._predict = _predict\n",
    "        self._predict_vmap = jax.vmap(_predict, in_axes=(None, 0))\n",
    "        if optimizer==None:\n",
    "            optimizer_instance = OGD(loss=batched_mse)\n",
    "            self._store_optimizer(optimizer_instance, self._predict)\n",
    "        else:\n",
    "            if scan_mode == False:\n",
    "                self._store_optimizer(optimizer, self._predict_vmap)\n",
    "            else:\n",
    "                self._store_optimizer(optimizer, self._predict)\n",
    "                \n",
    "\n",
    "        self.optimizer.initialize_metadata(self.params) # intialize state variables in optimizer to proper jax type\n",
    "\n",
    "        @jax.jit\n",
    "        def _predict_and_update(carry, xy):\n",
    "            params, metadata_opt, cnt = carry\n",
    "            pred = self._predict(params, xy[0])\n",
    "            metadata_opt['t'] = cnt\n",
    "            next_params, next_metadata_opt = self.optimizer.update_scan(params, metadata_opt, xy[0], xy[1])\n",
    "            if not (self.project_threshold is None):\n",
    "                for k, param in next_params.items():\n",
    "                    next_params[k] = jax.lax.cond(np.linalg.norm(next_params[k]) > self.project_threshold[k],\n",
    "                                                  None,\n",
    "                                                  lambda x : (self.project_threshold[k]/np.linalg.norm(next_params[k])) * next_params[k],\n",
    "                                                  next_params[k],\n",
    "                                                  lambda x : x)\n",
    "                                                  \n",
    "            return (next_params, next_metadata_opt, cnt+1), pred\n",
    "\n",
    "        self._predict_and_update = _predict_and_update\n",
    "\n",
    "    def predict_scan(self, x, params, metadata):\n",
    "        assert self.initialized\n",
    "        # print(\"x.shape = \" + str(x.shape))\n",
    "        # print(\"x = \" + str(x))\n",
    "        assert(x.shape[0] == self.l)\n",
    "        assert(x.shape[1] == self.n)\n",
    "        return self._predict(params, x)\n",
    "\n",
    "    def update_scan(self, params, metadata, metadata_opt, y):\n",
    "        assert self.initialized\n",
    "        ####TODO:Catch the error better\n",
    "        # assert self.x\n",
    "        next_params, next_metadata_opt = self.optimizer.update_scan(params, metadata_opt, metadata['x'], y)\n",
    "        if not (self.project_threshold is None):\n",
    "            for k, param in next_params.items():\n",
    "                next_params[k] = jax.lax.cond(np.linalg.norm(next_params[k]) > self.project_threshold[k],\n",
    "                                              None,\n",
    "                                              lambda x : (self.project_threshold[k]/np.linalg.norm(next_params[k])) * next_params[k],\n",
    "                                              next_params[k],\n",
    "                                              lambda x : x)\n",
    "        return next_params, metadata, next_metadata_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import tigerforecast\n",
    "from tigerforecast.utils.random import generate_key\n",
    "from tigerforecast.methods import Method\n",
    "from tigerforecast.utils.optimizers import *\n",
    "from tigerforecast.utils.optimizers.losses import *\n",
    "\n",
    "class Gradient_boosting(Method):\n",
    "    compatibles = set(['TimeSeries'])\n",
    "\n",
    "    def __init__(self, method_id, X_shape=(), Y_shape=(), loss=mse, eta=1.0, proxy_loss='original', W_update_rule='uniform', T=100):\n",
    "        \"\"\"\n",
    "        Description: Initializes autoregressive method parameters\n",
    "        Args:\n",
    "            method_id: list of instances of methods\n",
    "            X_shape: shape of input (exclude batch dim)\n",
    "            Y_shape: shape of output (exclude batch dim)\n",
    "            loss(function): loss function for boosting method\n",
    "            T: time horiizon\n",
    "        \"\"\"\n",
    "        self.X_shape = X_shape\n",
    "        self.Y_shape = Y_shape\n",
    "        self.N = len(method_id)\n",
    "        self.eta = eta\n",
    "        self.W_update_rule = W_update_rule\n",
    "        self.methods = []\n",
    "        self.all_params = [None for i in range(self.N)]  # params of each method\n",
    "        self.all_metadata = [None for i in range(self.N)] # metadata of each method\n",
    "        self.all_metadata_opt = [None for i in range(self.N)] # metadata of optimizer of each method\n",
    "        self.ys = []\n",
    "        self.Z = np.array([1.0/self.N for i in range(self.N)])\n",
    "        self.W = [1.0/self.N for i in range(self.N)]\n",
    "        self.preds = [0.0 for i in range(self.N)]\n",
    "\n",
    "        # proxy loss options\n",
    "        def original_loss(y_pred, yprev_ytrue):\n",
    "            y_prev, y_true = yprev_ytrue\n",
    "            y_true = np.reshape(y_true, self.Y_shape)\n",
    "            y_pred = np.reshape(y_pred, self.Y_shape)\n",
    "            return loss(y_prev + y_pred, y_true)\n",
    "\n",
    "        self._proxy_loss = original_loss\n",
    "\n",
    "        for i in range(self.N):\n",
    "            new_method = method_id[i]\n",
    "            new_method.optimizer.set_loss(self._proxy_loss)\n",
    "            self.methods.append(new_method)\n",
    "            self.all_params[i] = self.methods[i].params\n",
    "            self.all_metadata[i] = self.methods[i].metadata\n",
    "            self.all_metadata_opt[i] = self.methods[i].optimizer.metadata\n",
    "\n",
    "        def loss_W(W, preds, y):\n",
    "            pred = 0\n",
    "            for j in range(self.N):\n",
    "                pred += W[j] * preds[j]\n",
    "            return loss(pred, y)\n",
    "        self.loss_W = jax.jit(loss_W)\n",
    "\n",
    "        def _predict_scan(x, W, all_params, all_metadata):\n",
    "            ys = [np.zeros(self.Y_shape)]\n",
    "            method_preds = []\n",
    "            for i in range(self.N):\n",
    "                pred_i = np.reshape(self.methods[i].predict_scan(x, all_params[i], all_metadata[i]), self.Y_shape)\n",
    "                method_preds.append(pred_i)\n",
    "                y_i = ys[-1] + W[i] * pred_i\n",
    "                y_i = np.reshape(y_i, self.Y_shape)\n",
    "                ys.append(y_i)\n",
    "            return ys, method_preds\n",
    "\n",
    "        self._predict_scan = jax.jit(_predict_scan)\n",
    "\n",
    "        def _predict_and_update(carry, xy):\n",
    "            x, y = xy[0], xy[1]\n",
    "            Z, W, all_params, all_metadata, all_metadata_opt, cnt = carry\n",
    "            ys, method_yhats = self._predict_scan(x, W, all_params, all_metadata)\n",
    "            pred = ys[-1]\n",
    "\n",
    "            nums, den = None, None\n",
    "            \n",
    "            num = lambda j : W[j] * np.exp(-1 * self.eta * jax.grad(self.loss_W)(W, method_yhats, y)[j])\n",
    "            nums = [num(j) for j in range(self.N)]\n",
    "            den = np.sum(np.array(nums))\n",
    "            W = [nums[i]/den for i in range(self.N)]\n",
    "\n",
    "            for i in range(self.N):\n",
    "                # update method's self.x with proper shape (often needed for update step)\n",
    "                all_metadata[i]['x'] = np.reshape(x, all_metadata[i]['x'].shape)\n",
    "                all_metadata_opt[i]['t'] = cnt\n",
    "                all_params[i], all_metadata[i], all_metadata_opt[i] = self.methods[i].update_scan(all_params[i], \n",
    "                                                                                                  all_metadata[i], \n",
    "                                                                                                  all_metadata_opt[i], \n",
    "                                                                                                  (ys[i],y))\n",
    "            return (Z, W, all_params, all_metadata, all_metadata_opt, cnt+1), pred\n",
    "        self._predict_and_update = _predict_and_update\n",
    "\n",
    "    def predict_and_update(self, X, Y):\n",
    "        assert(X.shape[1:] == self.X_shape)\n",
    "        assert(Y.shape[1:] == self.Y_shape)\n",
    "        carry, preds = jax.lax.scan(self._predict_and_update, \n",
    "                                (self.Z, self.W, self.all_params, self.all_metadata, self.all_metadata_opt, 0), \n",
    "                                (X, Y)\n",
    "                               )\n",
    "        preds = np.reshape(preds, Y.shape)\n",
    "        W = carry[1]\n",
    "        # print(\"W = \" + str(W))\n",
    "        return preds, np.array(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/531 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01022500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/531 [00:02<20:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48434398\n",
      "01031500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/531 [00:04<40:23,  4.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7289c79b369f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#     loss, nse = float(loss), float(nse)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/skgaip/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_forward_method\u001b[0;34m(attrname, self, fun, *args)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0m_forward_to_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_forward_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/skgaip/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_lexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_force\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for basin in tqdm.tqdm(basins):\n",
    "    SEQUENCE_LENGTH = 270\n",
    "    INPUT_DIM = 32\n",
    "\n",
    "    b_threshold = 1e-4\n",
    "    eta = 0.008\n",
    "\n",
    "    W_lr_best_pairs = [\n",
    "            (0.03, 2e-5),\n",
    "            (0.05, 2e-5),\n",
    "            (0.07, 2e-5),\n",
    "            (0.09, 2e-5),\n",
    "        ]\n",
    "\n",
    "    method_ids = []\n",
    "\n",
    "    for W_threshold, lr in W_lr_best_pairs:\n",
    "        project_threshold = {\"W_lnm\": W_threshold, \"b\": b_threshold}\n",
    "        optim_ar = SGD(loss=batched_mse, learning_rate=lr, clip_grad=False)\n",
    "        method_ar = ARStateless_scan(\n",
    "            n=INPUT_DIM,\n",
    "            m=1,\n",
    "            l=SEQUENCE_LENGTH,\n",
    "            optimizer=optim_ar,\n",
    "            project_threshold=project_threshold,\n",
    "            scan_mode=True,\n",
    "        )\n",
    "        method_ids.append(method_ar)\n",
    "\n",
    "    method_boosting = Gradient_boosting(\n",
    "        method_ids,\n",
    "        X_shape=(270, 32),\n",
    "        Y_shape=(),\n",
    "        loss=batched_mse,\n",
    "        eta=eta,\n",
    "        proxy_loss=\"original\",\n",
    "        W_update_rule=\"GECO\",\n",
    "    )\n",
    "\n",
    "    yhats_LSTM = np.array(basin_to_yhats_LSTM[basin])\n",
    "    X = pickle.load(open(\"../data/flood/test/{}.pkl\".format(basin), \"rb\"))\n",
    "    Y = pickle.load(open(\"../data/flood/qobs/{}.pkl\".format(basin), \"rb\"))\n",
    "    \n",
    "    y_true = Y - yhats_LSTM\n",
    "    y_pred_ar, W = method_boosting.predict_and_update(X, y_true)\n",
    "    yhats = yhats_LSTM + y_pred_ar.squeeze()\n",
    "\n",
    "#     W_entropy = float(-1 * np.sum(W * np.log2(W)))\n",
    "\n",
    "    loss = ((Y - yhats) ** 2).mean()\n",
    "#     ys_mean = Y.mean()\n",
    "\n",
    "#     nse = 1 - ((Y - yhats) ** 2).sum() / ((Y - ys_mean) ** 2).sum()\n",
    "#     loss, nse = float(loss), float(nse)\n",
    "    \n",
    "    print(basin, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
